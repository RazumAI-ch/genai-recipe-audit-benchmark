# ðŸ§ª GenAI Recipe Audit Benchmark

A benchmark to evaluate how well Generative AI models (e.g. GPT-4o, Claude, Gemini, Mistral, LLaMA, etc.) are able to identify and classify ALCOA+ deviations.

A **diverse mix of models** are evaluated including:
- Proprietary closed-source LLMs (e.g., OpenAI GPT-4o, Gemini, Claude)
- Open-source foundation models (e.g., Mistral, LLaMA)
- **Our own LoRA-trained variants** fine-tuned on structured GxP data

---

## ðŸ“Š GxP Scoring

| Score     | Scope                     | Availability  |
|-----------|---------------------------|---------------|
| **GxP1**  | ALCOA+ violation detection | âœ… Open source |
| **GxP2**  | Recipe logic consistency   | ðŸ”’ Commercial  |
| **GxP3**  | Execution trace validation | ðŸ”’ Commercial  |

GxP1 detects timestamp issues, missing operators, overwritten fields, etc., with per-model scores normalized to [0â€“1].

---

## ðŸ”§ Capabilities

- Generate structured samples with injected deviations  
- Evaluate across OpenAI, Claude, Gemini, Mistral, etc.  
- Benchmark open models â€” including our own **LoRA-trained fine-tunes**  
- Compare quality, speed, and price across LLMs  
- Fully reproducible via Docker + PostgreSQL + Makefile  

---

## ðŸ“‚ Project Structure

```
genai-recipe-audit-benchmark/
â”œâ”€â”€ db/            # PostgreSQL schema + seed data
â”œâ”€â”€ scripts/       # Training data generation and LoRA export
â”œâ”€â”€ models/        # LoRA adapters and full fine-tuned models
â”œâ”€â”€ benchmark/     # Core scoring logic and run orchestration
â”œâ”€â”€ config/        # Prompt templates and pricing info
â”œâ”€â”€ llms/          # LLM wrappers (OpenAI, Gemini, etc.)
â”œâ”€â”€ docs/          # CLI usage and GxP methodology
â””â”€â”€ main.py        # Benchmark entry point
```

---

## ðŸš€ Run Locally

```bash
docker-compose up -d
make setup-db
python main.py
```

This runs a benchmark across all configured models, stores results, and outputs per-model cost + GxP1 scores.

---

## ðŸ’¼ Commercial Extensions

GxP2 and GxP3 are available via AICloudConsulting as part of a benchmark pilot for regulated clients.  
Use them to evaluate model alignment with real-world recipes and execution logs.

ðŸ‘‰ [AICloudConsulting.com](https://aicloudconsulting.com)

---

Â© 2025 AICloudConsulting â€” All rights reserved.