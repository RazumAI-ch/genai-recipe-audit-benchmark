# ğŸ§ª GenAI Recipe Audit Benchmark

A production-grade benchmark framework for evaluating how well Generative AI models (e.g., OpenAI GPT-4o, Claude, Gemini) detect compliance issues in structured healthcare manufacturing recipes.

Designed for highly regulated environments, this benchmark enables model comparison based on their ability to detect GxP-relevant issues â€” starting with ALCOA+ and extending (in commercial use) to full process-level evaluation.

---

## ğŸ“Š GxP Scoring â€” Core Output of the Benchmark

The benchmark generates **GxP scores** to measure model performance across multiple dimensions of regulatory relevance:

| Score     | Scope                        | Availability |
|-----------|------------------------------|--------------|
| **GxP1**  | ALCOA+ deviation detection    | âœ… Open source |
| **GxP2**  | Structured recipe logic       | ğŸ”’ Commercial |
| **GxP3**  | Execution trace evaluation    | ğŸ”’ Commercial |

### âœ… GxP1 â€” ALCOA+ Checks (Live)
Detects known violations of ALCOA+ principles such as:
- Inaccurate timestamps
- Missing or incorrect operator entries
- Overwritten or inconsistent status fields

Each model receives a normalized GxP1 score (0â€“1), reflecting its ability to catch these intentionally injected issues.

---

### ğŸ”’ GxP2 & GxP3 â€” For Real-World Data (Commercial)
Rather than relying on simulated complexity, GxP2 and GxP3 are reserved for evaluation using **your actual recipe and execution data**.

- **GxP2** evaluates logical consistency in multi-step recipes (e.g., sequence, duration, materials)
- **GxP3** compares recorded execution logs against expected recipe flows

> These tiers are available through AICloudConsulting as part of a commercial benchmark offering.

---

### ğŸ§  Unified GxP Score (Planned)
A future `GxP Score` may combine tiers into a weighted composite:

```text
GxP Score = w1 Ã— GxP1 + w2 Ã— GxP2 + w3 Ã— GxP3
```

This allows simple performance comparison while preserving detailed per-tier diagnostics.

---

## ğŸ” Core Capabilities

- Generate synthetic recipe records with embedded issues  
- Run benchmark evaluations across multiple GenAI models  
- Score results based on GxP1 (ALCOA+ detection)  
- Compare cost, speed, and quality across models  
- Track all runs with full reproducibility  
- Use results to guide model selection or integration decisions  

---

## ğŸ§  Key Features

- âœ… Model-agnostic architecture (OpenAI, Claude, Gemini, etc.)  
- âœ… Modular scoring logic by deviation type  
- âœ… Controlled fault injection with traceable metadata  
- âœ… Per-run cost tracking (token or infra-based)  
- âœ… Fully timestamped results for audit readiness  
- âœ… Designed for long-term automated benchmarking  

---

## ğŸ“‚ Project Structure

```
genai-recipe-audit-benchmark/
â”œâ”€â”€ main.py                # CLI entry point for running a full benchmark
â”œâ”€â”€ db/                    # PostgreSQL schema, migrations, utilities
â”‚   â””â”€â”€ schema.sql         # GxP1-focused schema with cost + score tracking
â”œâ”€â”€ benchmark/             # Core logic (BenchmarkRunner, scoring, orchestration)
â”œâ”€â”€ llms/                  # LLM interface layer + OpenAI, Gemini implementations
â”œâ”€â”€ config/                # Cost and model config (e.g., pricing.yaml)
â”œâ”€â”€ docs/                  # GxP scoring logic, methodology
â”œâ”€â”€ Makefile               # Utility tasks: setup-db, reset-db, run
â””â”€â”€ README.md              # This file
```

---

## ğŸš€ How to Run

```bash
pip install -r requirements.txt
make setup-db
python main.py
```

This will:
- Create or reuse the current benchmark sample
- Load configured LLMs
- Run evaluation excluding self-generated records
- Store token counts, timing, and normalized scores
- Output per-model cost and GxP1 quality metrics

---

## ğŸ“„ Output Includes

- âœ… Timestamped run metadata  
- âœ… Full list of models used  
- âœ… Cost breakdown per model (token-based or infra-based)  
- âœ… GxP1 quality score (normalized, 0â€“1)  
- âœ… JSON logs with record-level traceability  
- ğŸ”’ Optional: GxP2 and GxP3 scoring with commercial data

---

## âš–ï¸ License and Reuse

This project is released under the [Apache License 2.0](./LICENSE).

The benchmark is transparent and reproducible, but large-scale use may require substantial cloud/API compute.  
If you use this project:

- Please credit the original author  
- Link to this repository  
- Clearly note any scoring or data model changes  

For collaboration, co-authorship, or enterprise services, visit:  
[AICloudConsulting.com](https://aicloudconsulting.com)

---

## ğŸ’¼ Commercial Services

If your organization wants to:
- Benchmark GenAI models on your actual recipes and execution data
- Automate future GxP checks using the best-performing models
- Receive custom scoring, integration support, and audit reports

ğŸ‘‰ [Contact AICloudConsulting](https://aicloudconsulting.com) to start a tailored benchmark pilot.

---

Â© 2025 [AICloudConsulting.com](https://aicloudconsulting.com) â€” All rights reserved.